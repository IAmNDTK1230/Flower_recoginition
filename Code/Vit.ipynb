{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2271054,
          "sourceType": "datasetVersion",
          "datasetId": 76785
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "from keras.applications import (\n",
        "    ResNet50,\n",
        "    EfficientNetB0,\n",
        ")\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, top_k_accuracy_score\n",
        "import seaborn as sns\n",
        "import wandb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:12:52.769974Z",
          "iopub.execute_input": "2026-01-26T05:12:52.770394Z",
          "iopub.status.idle": "2026-01-26T05:13:28.808513Z",
          "shell.execute_reply.started": "2026-01-26T05:12:52.770363Z",
          "shell.execute_reply": "2026-01-26T05:13:28.807698Z"
        },
        "id": "bAsOCynl9dH8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nunenuh/pytorch-challange-flower-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dZW5gKJ-y8a",
        "outputId": "ad0b7c78-5635-428c-d74c-e2ffcc5e1931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'pytorch-challange-flower-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/pytorch-challange-flower-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(path)\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7YlwGVBG7M5",
        "outputId": "dc5d85c9-e448-460c-9e8a-75cb9cdd4c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/pytorch-challange-flower-dataset\n",
            "['sample_submission.csv', 'dataset', 'README.md', 'cat_to_name.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16   # Kaggle T4 an toàn hơn 32\n",
        "\n",
        "BASE_PATH = \"/kaggle/input/pytorch-challange-flower-dataset/dataset\"\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(BASE_PATH, \"train\"),\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode=\"int\"\n",
        ")\n",
        "\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(BASE_PATH, \"valid\"),\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode=\"int\"\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:13:31.074618Z",
          "iopub.execute_input": "2026-01-26T05:13:31.074935Z",
          "iopub.status.idle": "2026-01-26T05:13:31.080122Z",
          "shell.execute_reply.started": "2026-01-26T05:13:31.074907Z",
          "shell.execute_reply": "2026-01-26T05:13:31.079372Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ld1fLi9dH9",
        "outputId": "69f32a11-42e0-449e-f456-d12073e39ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6552 files belonging to 102 classes.\n",
            "Found 818 files belonging to 102 classes.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def normalize(x, y):\n",
        "    return tf.cast(x, tf.float32) / 255.0, y\n",
        "\n",
        "train_ds = train_ds.map(normalize).shuffle(1000).prefetch(AUTOTUNE)\n",
        "val_ds   = val_ds.map(normalize).prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:13:54.972391Z",
          "iopub.execute_input": "2026-01-26T05:13:54.972687Z",
          "iopub.status.idle": "2026-01-26T05:13:54.979074Z",
          "shell.execute_reply.started": "2026-01-26T05:13:54.972661Z",
          "shell.execute_reply": "2026-01-26T05:13:54.978393Z"
        },
        "id": "WvhIIdXS9dH-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.model_download(\n",
        "    \"keras/vit/keras/vit_base_patch16_224_imagenet\"\n",
        ")\n",
        "print(path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:13:57.118416Z",
          "iopub.execute_input": "2026-01-26T05:13:57.119116Z",
          "iopub.status.idle": "2026-01-26T05:15:21.954711Z",
          "shell.execute_reply.started": "2026-01-26T05:13:57.119085Z",
          "shell.execute_reply": "2026-01-26T05:15:21.954038Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RYupJqh9dH-",
        "outputId": "a9e59f88-364e-486f-e8b9-8f9c0dd1c218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/vit/keras/vit_base_patch16_224_imagenet/3\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_hub\n",
        "from keras import layers\n",
        "backbone = keras_hub.models.ViTBackbone.from_preset(\n",
        "    \"vit_base_patch16_224_imagenet\"\n",
        ")\n",
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "x = backbone(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(102, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred, y_pred_probs):\n",
        "    \"\"\"Calculate all required metrics\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    top5_acc = top_k_accuracy_score(y_true, y_pred_probs, k=5)\n",
        "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "    return acc, top5_acc, f1_macro, f1_weighted\n",
        "\n",
        "def collect_predictions(model, dataset):\n",
        "    \"\"\"Collect predictions from dataset\"\"\"\n",
        "    y_true_all, y_pred_all, y_pred_probs_all = [], [], []\n",
        "    for x_batch, y_batch in dataset:\n",
        "        batch_probs = model.predict(x_batch, verbose=0)\n",
        "        batch_preds = np.argmax(batch_probs, axis=1)\n",
        "        y_true_all.extend(y_batch.numpy())\n",
        "        y_pred_all.extend(batch_preds)\n",
        "        y_pred_probs_all.extend(batch_probs)\n",
        "    return np.array(y_true_all), np.array(y_pred_all), np.array(y_pred_probs_all)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:16:08.660743Z",
          "iopub.execute_input": "2026-01-26T05:16:08.660996Z",
          "iopub.status.idle": "2026-01-26T05:16:52.576457Z",
          "shell.execute_reply.started": "2026-01-26T05:16:08.660972Z",
          "shell.execute_reply": "2026-01-26T05:16:52.575761Z"
        },
        "id": "P7aWrI2V9dH-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== ONE-CELL ViT FINETUNING (FREEZE → UNFREEZE) ======\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras import layers\n",
        "\n",
        "NUM_CLASSES = 102\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=\"flower-classification-vit\",\n",
        "    config={\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"architecture\": \"ViT-Base-Patch16-224\",\n",
        "        \"dataset\": \"Flowers102\",\n",
        "        \"epochs\": 10,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"img_size\": IMG_SIZE,\n",
        "        \"num_classes\": NUM_CLASSES\n",
        "    }\n",
        ")\n",
        "\n",
        "# 1. Load pretrained ViT backbone (ImageNet)\n",
        "backbone = keras_hub.models.ViTBackbone.from_preset(\n",
        "    \"vit_base_patch16_224_imagenet\"\n",
        ")\n",
        "\n",
        "# 2. PHASE 1: Freeze backbone\n",
        "backbone.trainable = False\n",
        "\n",
        "# 3. Build model\n",
        "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = backbone(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "wandb.config.update({\"model_summary\": str(model.summary())})\n",
        "\n",
        "# 4. Compile & train (head only)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "# Phase 1 Training with wandb logging\n",
        "print(\"Phase 1: Training head only (backbone frozen)\")\n",
        "for epoch in range(5):\n",
        "    print(f\"\\nEpoch {epoch+1}/5\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_batches = 0\n",
        "\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        # Train on batch\n",
        "        loss, acc = model.train_on_batch(x_batch, y_batch)\n",
        "        train_loss += loss\n",
        "        train_acc += acc\n",
        "        train_batches += 1\n",
        "\n",
        "        # Log batch metrics every 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            wandb.log({\n",
        "                \"phase1/batch_loss\": loss,\n",
        "                \"phase1/batch_accuracy\": acc,\n",
        "                \"batch\": epoch * len(train_ds) + batch_idx\n",
        "            })\n",
        "\n",
        "    # Calculate epoch averages\n",
        "    avg_train_loss = train_loss / train_batches\n",
        "    avg_train_acc = train_acc / train_batches\n",
        "\n",
        "    # Collect predictions for detailed metrics\n",
        "    y_true, y_pred, y_pred_probs = collect_predictions(model, val_ds)\n",
        "\n",
        "    # Calculate all metrics\n",
        "    acc, top5_acc, f1_macro, f1_weighted = calculate_all_metrics(y_true, y_pred, y_pred_probs)\n",
        "    # Validation\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    val_loss = 0\n",
        "    val_batches = 0\n",
        "    for x_batch, y_batch in val_ds:\n",
        "        loss, _ = model.test_on_batch(x_batch, y_batch)\n",
        "        val_loss += loss\n",
        "        val_batches += 1\n",
        "    avg_val_loss = val_loss / val_batches\n",
        "\n",
        "    # Log epoch metrics\n",
        "    wandb.log({\n",
        "        \"phase1/epoch\": epoch + 1,\n",
        "        \"phase1/train_loss\": avg_train_loss,\n",
        "        \"phase1/train_accuracy\": avg_train_acc,\n",
        "        \"phase1/val_loss\": avg_val_loss,\n",
        "        \"phase1/val_accuracy\": acc,\n",
        "        \"phase1/val_top5_accuracy\": top5_acc,\n",
        "        \"phase1/val_f1_macro\": f1_macro,\n",
        "        \"phase1/val_f1_weighted\": f1_weighted,\n",
        "        \"learning_rate\": 1e-3\n",
        "    })\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(\"\\nEvaluation Results (Validation Set)\")\n",
        "    print(f\"Accuracy (Top-1) : {acc:.4f}\")\n",
        "    print(f\"Top-5 Accuracy  : {top5_acc:.4f}\")\n",
        "    print(f\"F1-score macro  : {f1_macro:.4f}\")\n",
        "    print(f\"F1-score weight : {f1_weighted:.4f}\")\n",
        "\n",
        "# 5. PHASE 2: Unfreeze top ViT layers\n",
        "for layer in backbone.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 6. Re-compile with small LR\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# 7. Fine-tune with wandb logging\n",
        "print(\"\\nPhase 2: Fine-tuning top layers\")\n",
        "for epoch in range(5):\n",
        "    print(f\"\\nEpoch {epoch+1}/5 (Phase 2)\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_batches = 0\n",
        "\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        # Train on batch\n",
        "        loss, acc = model.train_on_batch(x_batch, y_batch)\n",
        "        train_loss += loss\n",
        "        train_acc += acc\n",
        "        train_batches += 1\n",
        "\n",
        "        # Log batch metrics every 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            wandb.log({\n",
        "                \"phase2/batch_loss\": loss,\n",
        "                \"phase2/batch_accuracy\": acc,\n",
        "                \"batch\": (epoch + 5) * len(train_ds) + batch_idx,\n",
        "                \"phase2_epoch\": epoch + 1\n",
        "            })\n",
        "\n",
        "    # Calculate epoch averages\n",
        "    avg_train_loss = train_loss / train_batches\n",
        "    avg_train_acc = train_acc / train_batches\n",
        "\n",
        "    # Collect predictions for detailed metrics\n",
        "    y_true, y_pred, y_pred_probs = collect_predictions(model, val_ds)\n",
        "\n",
        "    # Calculate all metrics\n",
        "    acc, top5_acc, f1_macro, f1_weighted = calculate_all_metrics(y_true, y_pred, y_pred_probs)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = 0\n",
        "    val_batches = 0\n",
        "    for x_batch, y_batch in val_ds:\n",
        "        loss, _ = model.test_on_batch(x_batch, y_batch)\n",
        "        val_loss += loss\n",
        "        val_batches += 1\n",
        "    avg_val_loss = val_loss / val_batches\n",
        "    # Log epoch metrics\n",
        "    wandb.log({\n",
        "        \"phase2/epoch\": epoch + 1,\n",
        "        \"phase2/train_loss\": avg_train_loss,\n",
        "        \"phase2/train_accuracy\": avg_train_acc,\n",
        "        \"phase2/val_loss\": avg_val_loss,\n",
        "        \"phase2/val_accuracy\": acc,\n",
        "        \"phase2/val_top5_accuracy\": top5_acc,\n",
        "        \"phase2/val_f1_macro\": f1_macro,\n",
        "        \"phase2/val_f1_weighted\": f1_weighted,\n",
        "        \"learning_rate\": 1e-5\n",
        "    })\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(\"\\nEvaluation Results (Validation Set)\")\n",
        "    print(f\"Accuracy (Top-1) : {acc:.4f}\")\n",
        "    print(f\"Top-5 Accuracy  : {top5_acc:.4f}\")\n",
        "    print(f\"F1-score macro  : {f1_macro:.4f}\")\n",
        "    print(f\"F1-score weight : {f1_weighted:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:26:05.905876Z",
          "iopub.execute_input": "2026-01-26T05:26:05.906627Z",
          "iopub.status.idle": "2026-01-26T05:26:05.915016Z",
          "shell.execute_reply.started": "2026-01-26T05:26:05.906595Z",
          "shell.execute_reply": "2026-01-26T05:26:05.914172Z"
        },
        "id": "R1zKmntr9dH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0af83079-a85a-4298-f649-844d5b4cd240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mblud\u001b[0m (\u001b[33mblud-fpt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260130_164625-6dwnrfmj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/blud-fpt-university/flower-classification-vit/runs/6dwnrfmj' target=\"_blank\">vital-frog-4</a></strong> to <a href='https://wandb.ai/blud-fpt-university/flower-classification-vit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/blud-fpt-university/flower-classification-vit' target=\"_blank\">https://wandb.ai/blud-fpt-university/flower-classification-vit</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/blud-fpt-university/flower-classification-vit/runs/6dwnrfmj' target=\"_blank\">https://wandb.ai/blud-fpt-university/flower-classification-vit/runs/6dwnrfmj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vi_t_backbone (\u001b[38;5;33mViTBackbone\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m85,798,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m393,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102\u001b[0m)            │        \u001b[38;5;34m52,326\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vi_t_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ViTBackbone</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">85,798,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,326</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m86,244,710\u001b[0m (329.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,244,710</span> (329.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m446,054\u001b[0m (1.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">446,054</span> (1.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m85,798,656\u001b[0m (327.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,798,656</span> (327.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Training head only (backbone frozen)\n",
            "\n",
            "Epoch 1/5\n",
            "Train Loss: 1.2875, Train Acc: 0.7433\n",
            "Val Loss: 0.4641\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9902\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9841\n",
            "F1-score weight : 0.9902\n",
            "\n",
            "Epoch 2/5\n",
            "Train Loss: 0.3269, Train Acc: 0.9329\n",
            "Val Loss: 0.2463\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9927\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9885\n",
            "F1-score weight : 0.9927\n",
            "\n",
            "Epoch 3/5\n",
            "Train Loss: 0.2046, Train Acc: 0.9579\n",
            "Val Loss: 0.1737\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9914\n",
            "Top-5 Accuracy  : 0.9976\n",
            "F1-score macro  : 0.9909\n",
            "F1-score weight : 0.9914\n",
            "\n",
            "Epoch 4/5\n",
            "Train Loss: 0.1535, Train Acc: 0.9680\n",
            "Val Loss: 0.1386\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9804\n",
            "Top-5 Accuracy  : 0.9976\n",
            "F1-score macro  : 0.9766\n",
            "F1-score weight : 0.9768\n",
            "\n",
            "Epoch 5/5\n",
            "Train Loss: 0.1285, Train Acc: 0.9731\n",
            "Val Loss: 0.1210\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9939\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9924\n",
            "F1-score weight : 0.9940\n",
            "\n",
            "Phase 2: Fine-tuning top layers\n",
            "\n",
            "Epoch 1/5 (Phase 2)\n",
            "Train Loss: 0.0164, Train Acc: 0.9962\n",
            "Val Loss: 0.0197\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9963\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9963\n",
            "F1-score weight : 0.9963\n",
            "\n",
            "Epoch 2/5 (Phase 2)\n",
            "Train Loss: 0.0190, Train Acc: 0.9964\n",
            "Val Loss: 0.0171\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9976\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9976\n",
            "F1-score weight : 0.9975\n",
            "\n",
            "Epoch 3/5 (Phase 2)\n",
            "Train Loss: 0.0172, Train Acc: 0.9967\n",
            "Val Loss: 0.0165\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9976\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9976\n",
            "F1-score weight : 0.9975\n",
            "\n",
            "Epoch 4/5 (Phase 2)\n",
            "Train Loss: 0.0157, Train Acc: 0.9972\n",
            "Val Loss: 0.0142\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9976\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9976\n",
            "F1-score weight : 0.9975\n",
            "\n",
            "Epoch 5/5 (Phase 2)\n",
            "Train Loss: 0.0138, Train Acc: 0.9977\n",
            "Val Loss: 0.0129\n",
            "\n",
            "Evaluation Results (Validation Set)\n",
            "Accuracy (Top-1) : 0.9976\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9976\n",
            "F1-score weight : 0.9975\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"vit_flowers102_finetuned.keras\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:28:52.030193Z",
          "iopub.execute_input": "2026-01-26T05:28:52.030994Z",
          "iopub.status.idle": "2026-01-26T05:28:52.038326Z",
          "shell.execute_reply.started": "2026-01-26T05:28:52.030950Z",
          "shell.execute_reply": "2026-01-26T05:28:52.037629Z"
        },
        "id": "dbWRRNl29dIA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "image_paths = sorted(\n",
        "    glob.glob(\"/kaggle/input/pytorch-challange-flower-dataset/dataset/test/*.jpg\")\n",
        ")\n",
        "\n",
        "def load_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "test_ds = test_ds.map(load_image).batch(BATCH_SIZE)\n",
        "\n"
      ],
      "metadata": {
        "id": "2ftgmtfk1_bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "model = keras.models.load_model(\"vit_flowers102_finetuned.keras\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-26T05:28:54.116182Z",
          "iopub.execute_input": "2026-01-26T05:28:54.116951Z",
          "iopub.status.idle": "2026-01-26T05:28:54.121967Z",
          "shell.execute_reply.started": "2026-01-26T05:28:54.116904Z",
          "shell.execute_reply": "2026-01-26T05:28:54.121258Z"
        },
        "id": "sT4Pe1hF9dIA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwCobuQU3VUM",
        "outputId": "989e45a5-d7f6-448a-a172-5af0db1b89ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 543ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on validation set\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/kaggle/input/pytorch-challange-flower-dataset/dataset/valid\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (x / 255.0, y))\n",
        "\n",
        "# Collect predictions for test set\n",
        "y_true, y_pred, y_pred_probs = collect_predictions(model, test_ds)\n",
        "\n",
        "# Calculate all metrics\n",
        "acc, top5_acc, f1_macro, f1_weighted = calculate_all_metrics(y_true, y_pred, y_pred_probs)\n",
        "\n",
        "# Calculate test loss\n",
        "test_loss = 0\n",
        "test_batches = 0\n",
        "for x_batch, y_batch in test_ds:\n",
        "    loss, _ = model.test_on_batch(x_batch, y_batch)\n",
        "    test_loss += loss\n",
        "    test_batches += 1\n",
        "avg_test_loss = test_loss / test_batches\n",
        "\n",
        "# Log final test metrics to wandb\n",
        "wandb.log({\n",
        "    \"final_test_loss\": avg_test_loss,\n",
        "    \"final_test_accuracy\": acc,\n",
        "    \"final_test_top5_accuracy\": top5_acc,\n",
        "    \"final_test_f1_macro\": f1_macro,\n",
        "    \"final_test_f1_weighted\": f1_weighted\n",
        "})\n",
        "\n",
        "# Print final evaluation results\n",
        "print(\"\\nEvaluation Results (Final Test Set)\")\n",
        "print(f\"Accuracy (Top-1) : {acc:.4f}\")\n",
        "print(f\"Top-5 Accuracy  : {top5_acc:.4f}\")\n",
        "print(f\"F1-score macro  : {f1_macro:.4f}\")\n",
        "print(f\"F1-score weight : {f1_weighted:.4f}\")\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "XbXm2L3l4vOD",
        "outputId": "d6ca3d66-21c9-47c5-8d2e-0fab3e69a82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "FINAL EVALUATION ON TEST SET\n",
            "==================================================\n",
            "Found 818 files belonging to 102 classes.\n",
            "\n",
            "Evaluation Results (Final Test Set)\n",
            "Accuracy (Top-1) : 0.9976\n",
            "Top-5 Accuracy  : 0.9988\n",
            "F1-score macro  : 0.9976\n",
            "F1-score weight : 0.9975\n",
            "Test Loss: 0.0057\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_f1_macro</td><td>▁</td></tr><tr><td>final_test_f1_weighted</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>final_test_top5_accuracy</td><td>▁</td></tr><tr><td>learning_rate</td><td>█████▁▁▁▁▁</td></tr><tr><td>phase1/batch_accuracy</td><td>▁▅▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>phase1/batch_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>phase1/epoch</td><td>▁▃▅▆█</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>4090</td></tr><tr><td>final_test_accuracy</td><td>0.99756</td></tr><tr><td>final_test_f1_macro</td><td>0.9976</td></tr><tr><td>final_test_f1_weighted</td><td>0.99753</td></tr><tr><td>final_test_loss</td><td>0.00575</td></tr><tr><td>final_test_top5_accuracy</td><td>0.99878</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>phase1/batch_accuracy</td><td>0.97456</td></tr><tr><td>phase1/batch_loss</td><td>0.12207</td></tr><tr><td>phase1/epoch</td><td>5</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vital-frog-4</strong> at: <a href='https://wandb.ai/blud-fpt-university/flower-classification-vit/runs/6dwnrfmj' target=\"_blank\">https://wandb.ai/blud-fpt-university/flower-classification-vit/runs/6dwnrfmj</a><br> View project at: <a href='https://wandb.ai/blud-fpt-university/flower-classification-vit' target=\"_blank\">https://wandb.ai/blud-fpt-university/flower-classification-vit</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260130_164625-6dwnrfmj/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}